{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f144fecb",
   "metadata": {},
   "source": [
    "The ReLU (Rectified Linear Unit) is one of the most commonly used activation functions in neural networks due to its simplicity and efficiency. It is defined as:  f(x)=max⁡(0,x)\n",
    "\n",
    "This means it ranges from [0, ∞) i.e for any input value x, it returns x if it is positive and 0 if it is negative. But this approach causes some issues.\n",
    "\n",
    "\n",
    "### Limitations of ReLU\n",
    "While ReLU is widely adopted it comes with some drawbacks especially during training deep networks:\n",
    "\n",
    "* Dead Neurons: If a neuron receives only negative inputs, it outputs zero and its gradient becomes zero. This means it stops learning.\n",
    "\n",
    "* Non-symmetric: ReLU does not treat negative and positive values equally which can slow down learning in some cases.\n",
    "\n",
    "* Exploding Activations: In some cases the activation values can become too large for large positive inputs.\n",
    "\n",
    "To overcome these limitations leaky relu activation function was introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c0bae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2084d65",
   "metadata": {},
   "source": [
    "###  What is Leaky ReLU?\n",
    "Leaky ReLU is a modified version of ReLU designed to fix the problem of dead neurons. Instead of returning zero for negative inputs it allows a small, non-zero value. It introduces a slight modification to the standard ReLU by assigning a small, fixed slope to the negative part of the input. This ensures that neurons don't become inactive during training as they can still pass small gradients even when receiving negative values.\n",
    "\n",
    "Its equation is:\n",
    "\n",
    "<img src=\"https://datagy.io/wp-content/uploads/2023/09/Screenshot-2023-09-13-at-7.51.18-AM.png\" width=\"480\">\n",
    "\n",
    "and its graph is:\n",
    "\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200516163110/lerelu.jpg\" width=\"480\">\n",
    "\n",
    "* X-axis: Input values to the activation function ranging from -5 to +2.\n",
    "* Y-axis: Output values of the Leaky ReLU function.\n",
    "\n",
    "\n",
    "​\n",
    "Uses of Leaky ReLU\n",
    "* Prevents dead neurons by allowing a small gradient for negative inputs.\n",
    "* Improves gradient flow during backpropagation.\n",
    "* Helps in faster and more stable training compared to ReLU.\n",
    "* Useful in deep networks where ReLU may fail.\n",
    "\n",
    "\n",
    "​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442293e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
